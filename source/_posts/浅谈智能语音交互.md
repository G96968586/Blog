---
title: 浅谈智能语音交互
date: 2018-04-04 11:19:37
categories: Android
tags: Android
---
## 前言

近两年，人工智能发展迅猛，语音识别作为人工智能最成熟的技术之一，现已成为很多智能设备甚至是 App 的标配。各大科技巨头也相继推出了具有代表性的智能语音产品，比如，国外有亚马逊的 [Amazon Echo](https://www.amazon.com/Amazon-Echo-Bluetooth-Speaker-with-WiFi-Alexa/dp/B00X4WHP5E)、微软的 [Cortana](https://www.microsoft.com/en-us/search/result.aspx?q=cortana#nav-downloads)、谷歌的 [Google Assistant](https://assistant.google.com/)、脸书的 [Facebook M](https://www.theguardian.com/technology/2015/aug/27/facebook-m-virtual-assistant-siri-google-now) 等，国内有阿里巴巴的 [天猫精灵](https://detail.tmall.com/item.htm?spm=a220m.1000858.1000725.6.676e5549peca3Q&amp;id=557856671085&amp;skuId=3572248181527&amp;user_id=3081047815&amp;cat_id=2&amp;is_b=1&amp;rn=ab5f34354c32a735c187eb22609ca729) 、百度的 [度秘](https://dueros.baidu.com/) 以及科大讯飞的 [智能语音输入法](http://www.xfyun.cn/) 等等。
<!-- more -->
目前，市面上最常见的语音交互使用场景还是语音助手。语音助手主要提供的是事务处理和知识服务。比如：询问天气预报、提供位置导航服务、推荐周边好吃的东西、播放想听的音乐、设定闹钟、问答等等，这些都是大家熟悉的不能再熟悉的例子了。有些语音助手，一出生就是为了解决某类行业的问题，就会推出专属领域的版本，比如百度地图的 "小度"，贴心解决用户的出行问题；阿里的小蜜，致力于成为会员的购物私人助理，等等。

在当今的移动互联网+大数据时代，智能语音交互技术将架起用户跟成千上万的互联云端服务之间的桥梁，成为下一个必争的入口。

## 语音交互能给用户带来巨大的体验优化

* 服务直达
    * 在设备上众多的 App 中通过语音操纵某个 App 完成一系列的相关操作。
        例如：对着手机说 "使用高德地图导航去阿里巴巴西溪园区"，手机就会打开高德地图，自动进入导航页面，开始播放语音导航指引；又或者对着手机说 "用支付宝给某某转账 100 元"，就可以直接打开支付宝，把转账金额和对象确定好，等待用户确认就可以完成转账。
    * 在客服服务中心通话过程中直接用语音直达适合自己的服务选项
        例如：用户致电淘宝客服说 “我要退货”，系统就会直接把用户转接到“退货退款”相关技能组，并把相关信息展示给客服小二。
    
* 操作便利
    * 穿戴设备如手环，手表，都有屏幕小，触控不便的弱点，加上语音交互必然会带来很大的便利性。
    * 蓝牙耳机，若内置语音识别功能，从此听歌切歌、调音量、接电话再也不需要手动操作。现在的耳机很多都带存储卡，通过简单的语音口令，控制播放本地音乐，摆脱对手机和网络的依赖。
    * 通过智能音箱或是其他的语音控制中心来操纵智能家具。
    
* 挖掘价值
    * 很多的录音数据很难直接被利用和分析，若通过语音识别将录音转成文字，就可以利用自然语言处理技术和算法模型来发掘价值了
    
* 改变智能客服机器人的交互方式
    * 市面上智能客服机器人已经很常见了，但大多都是文字聊天式的。如果能在电话通信服务这块加入语音识别交互，利用实时识别技术将通信语音转为文字，在服务端通过语义理解、语言模型处理和语音合成技术做到模拟真人回复用户，这将给智能客服机器人带来一种全新的交互方式。

## 语音识别原理

智能语音关键技术在语音识别这一块。顾名思义，语音识别是以语音作为研究对象，通过信号处理技术和模式识别让机器能够自动识别和理解人类口述的语言。

语音其实就是一种声波，怎么将这种波转换成机器可以识别的内容，这是第一步，也叫特征提取。但一般在特征提取之前，我们会对语音进行预处理，通过信号处理的一些技术对其收尾端的静音进行切除，降低对后续步骤造成的干扰。对静音切除的这一操作，一般叫做 [VAD](https://en.wikipedia.org/wiki/Voice_activity_detection)。如下面波形图所示：

![image | center | 520x132](https://gw.alipayobjects.com/zos/skylark/a8aa217b-c19c-4ff7-bf6c-8756e1be58b9/2018/png/bd4ef7d2-d4ff-4e36-bbf0-ddae1214c417.png "")


这里我们对波的处理必须是未经过压缩的纯波形文件，像 mp3、ogg 等因为是压缩过的格式文件，所以是不能被选为波处理输入源文件。这也就是为什么我们在进行模型训练时都选择 [wav](https://en.wikipedia.org/wiki/WAV) 格式的音频文件。wav 文件除了文件头携带的一些信息外，剩下的就是声音波形的一个个纯点了。接下来，需要对声音进行分帧，即把声音分成一小段一小段，每一小段我们称为一帧。分帧的操作是通过移动 [窗函数](https://en.wikipedia.org/wiki/Window_function) 来实现的。一般帧与帧之间存在着交叠，如下图红色区块所示：

![image | center | 415x216](https://gw.alipayobjects.com/zos/skylark/a639a306-1e1a-432b-959c-f90b6e541c18/2018/png/7afea443-3142-41ff-a1ec-7328a10a6057.png "")


可以看出，每一帧的长度为 25 ms，红色部分为交叠部分，长度为 15 ms，即帧移为 10 ms。对于这种分帧，我们称为以帧长25ms、帧移 10ms 分帧。分帧后，语音变成了很多小段。由于分帧后的波形在时域上几乎没描述能力，接着还需对其波形进行变换，这一过程叫做声学特征提取。常见的一种变换方法是提取 MFCC 特征。根据人耳的生理特性，把每一帧波形变成一个多维向量，我们可以简单地理解为这个向量包含了这帧语音的内容信息。假设声学特征是 12 维，声音就成了一个 12 行、N（总帧数）列的一个矩阵，我们称之为观察序列。如下图所示，每一帧都用一个 12 维的向量表示，色块的颜色深浅表示向量值的大小。

![image | center | 225x237](https://gw.alipayobjects.com/zos/skylark/c55ebbde-4618-4737-a595-56dc2657c2d0/2018/png/162b512d-f482-49ff-9530-b336682ba86c.png "")


到此，特征提取已经完成。接下来就要考虑怎样把这个矩阵变成文本，即建立语音模版。先了解两个概念：

* 音素：单词的发音由音素构成。对英语，一种常用的音素集是卡内基梅隆大学的一套由 39 个音素构成的音素集，具体可参见 [The CMU Pronouncing Dictionary](http://www.speech.cs.cmu.edu/cgi-bin/cmudict)。汉语一般直接用全部声母和韵母作为音素集，另外，汉语识别还分有调无调，这里不进一步详述。

* 状态：简单理解成比音素更细致的语音单位，通常把一个音素划分成 3 个状态。

语音识别通常是这样工作的，将帧识别成状态，这一步是难点，然后把状态组合成音素，再把音素组合成单词，我们来看下面这张图：

![image | center | 361x205](https://gw.alipayobjects.com/zos/skylark/65cbd905-adf7-41f5-ad48-1c9e36116831/2018/png/ae27524f-adbf-4047-bb89-9640575daf31.png "")


图中每个竖条代表一帧，S1029、S124 等是状态，ay 是音素。可以发现，状态由若干帧组合成，音素则由三个状态组合成。现在，我们可以知道，只需要能找到每一帧所对应的状态，也就可以识别出结果了。有一个方法，就是看帧对应哪个状态的概率最大，那这帧就属于那个状态（注：一帧可以对应多个状态）。
![image | center](https://gw.alipayobjects.com/zos/skylark/b1e344b0-8d52-4038-9cfd-334935359769/2018/png/ae20359d-3de4-47eb-87e6-edc087e376fc.png "")

上图一帧对应了 S1 ～ S5 五个状态，可以看出该帧在 S3 所处的概率最大，所以该帧的状态就为 S3。那这些状态的概率是哪来的呢？这就要提到模型库中的声学模型了。声学模型里有一大堆的参数，从这些参数里可以找到帧与状态对应的概率。这一大堆的参数是通过海量的语音数据训练得到的。

现在又有一个新的问题，因为每一帧都会得到一个状态号，所以最后整个语音就会得到一堆乱七八糟的状态号，相邻两帧间的状态号基本都不相同。假设语音有 1000 帧，每帧对应 1 个状态，每 3 个状态组合成一个音素，那么大概会组合成 300 个音素，但这段语音其实根本没有这么多音素。如果真这么做，得到的状态号可能根本无法组合成音素。实际上，相邻帧的状态应该大多数都是相同的才合理，因为每帧很短。解决这个问题需要用到隐马尔可夫模型，求最大似然状态路径。对于不了解隐马尔可夫模型的同学，可以参见这篇知乎问答[《如何用简单易懂的例子解释隐马尔可夫模型》](https://www.zhihu.com/question/20962240)，这里不做过多解释。

在语音识别中应用隐马尔可夫模型的整个过程，我们称之为“解码”。首先，需要搭建一个状态网络（由单词级网络展开成音素网络，再由音素网络展开称状态网络）。如果我们要识别任意的文本，则必须要保证这个状态网络足够的大，但这个网络越大，想要达到比较好的识别准确率就越难。所以要根据实际任务的需求，合理选择网络大小和结构。其次，则是在这个状态网络中寻找与声音最匹配的路径，语音识别过程其实就是在状态网络中搜索一条最佳路径，语音对应这条路径的概率最大。

前面所说的概率，由三部分构成，分别是：观察概率（每帧和每个状态对应的概率）、转移概率（每个状态转移到自身或转移到下个状态的概率）和语言概率（根据语言统计规律得到的概率）。前两种概率从声学模型中获取，最后一种概率从语言模型中获取。语言模型是使用大量的文本训练出来的，可以利用某门语言本身的统计规律来帮助提升识别正确率。语言模型很重要，如果不使用语言模型，当状态网络较大时，识别出的结果基本是一团遭。
至此，语音识别的过程就完成了。

简单的总结，语音从输入到被识别出来，会经历：语音信号预处理 -> 特征提取 -> 模式匹配 -> 参考模型库 -> 识别结果。基本结构如下图所示：
![image | center](https://gw.alipayobjects.com/zos/skylark/23156966-fda4-45f7-a413-015c79eb79e1/2018/png/69d5f6b4-01f5-4845-8531-567882a58c7f.png "")

## 业务思考

利用创新技术去推动优化产品，充分发挥技术的价值。这里以 [阿里众包](http://h5.m.taobao.com/job/cloud-work/index.html) 为例，我们可以利用创新技术去做哪些改进。

阿里众包有 B、C 端两个产品，B 端用于发布工作，招募人员来完成工作并给出约定的报酬。C 端用户可以根据自己的兴趣去报名工作，然后申领任务并完成，最终拿取薪资报酬。众包有一种语音类型的工作，这类工作主要是收集符合条件的语音用于特定模型的训练。举个例子，如图所示：

![image | center | 290x512](https://gw.alipayobjects.com/zos/skylark/d8a74614-c392-43c9-9a4e-212676868b03/2018/png/533f4c96-055f-4020-8bc6-535e19515359.png "")


有商家发布了《粤语朗读-xc》的语音任务，上面有该任务的报名基本要求，工作简介里面写的是任务要求，也就是用户在录音时需要注意的地方。现在我已经成功报了名，并完成了一些任务，正处于等待任务验收的阶段，等众包工作人员审核通过后，我就可以拿到属于我的报酬了。

![image | center | 289x511](https://gw.alipayobjects.com/zos/skylark/909097ce-f5c2-48c5-9d7d-2168a584e85d/2018/png/3e322ea7-b2e2-4138-9d02-096a0801e562.png "")


然而，在这个流程当中存在着两个痛点，一是产品前端保证不了用户按照任务要求录入语音，这样会给后端的质检工作人员带来额外的工作量，导致任务质检的时间增多。下图是当前用户端做任务的流程图：

![老流程.png | left | 650x40](https://cdn.yuque.com/lark/2018/png/16729/1522501096484-f26337bb-4e9a-41a1-b11a-f37a8584c483.png "")

二是任务的数量原本就是有限的，这样会降低任务的完成率，导致后面商家不得不再重新发布一次任务收集语音样本，造成了额外的时间和人力成本。

我们是否可以借助智能语音识别技术，去做一些流程上的优化呢？拿前面的流程图，稍微做如下的改造，
![a.png | center | 619x357](https://cdn.yuque.com/lark/2018/png/16729/1522502253994-af13d286-7a30-40f4-80e0-126f6aab4997.png "")


我们在产品前端加上了语音识别模块，在用户提交结果前先把用户朗读的语音样本转为文本，做一次样本质量校验，比如，判断用户是否读错字或者漏字，或者拉长音、咬字不清等。如果不符合任务要求，则提示用户重做任务；如果部分不符合要求，比如存在停顿、噪音等问题，我们可以通过技术去优化。这样既提高了产品使用体验，也大大提高了任务的完成率和后面的质检效率，一举多得。

当然，由于任务的工种很多，有一些工种是不需要对用户的语音输入文本进行校验的，业务方要的就是参差不齐的语料，所以我们还得在前面增加一个流程，控制是否走语音校验。

另外，众包也可以考虑推出自己的专属产品智能语音助手，在用户做任务的过程中提供全方位的智能服务。比如：用户可以直接通过询问众包助手附近是否有可以做的任务，众包助手就可以根据用户的需求给他们筛选出距离近薪资高的任务并推荐给用户；又比如众包助手可以每天分析天气情况，若查到今天会下雨，提前提示用户备好雨具；在用户做任务的过程中，给出一些关心的贴心话语，比如“亲，您已经连续做任务2个小时了，该休息下啦”、“亲，月底已到，小包提醒您来看看这个月的收入啦”等等，增加产品的亲合力。我们有很多的地方都可以去做得更好。

## 技术支持

现在市面上有很多成熟的智能语音技术解决方案，想要让我们的产品支持智能语音能力门槛不高，这里以阿里云提供的语音服务为例，简单跟大家介绍一下。因为本人是 Android 开发者，所以下面内容以 Android Demo 为例。

使用阿里云的服务前，你需要先注册阿里云账号并开通智能语音交互 [服务](https://data.aliyun.com/product/nls?spm=5176.doc30437.2.4.V1fz8y)，获取Access Key ID和Access Key Secret两个数加认证参数。详情可参见文档[《账号和服务申请》](https://help.aliyun.com/document_detail/30437.html?spm=5176.doc30422.2.4.5ArQSd)。

阿里云的语音识别提供了三种服务：一句话识别、实时语音识别和录音文件识别。具体可根据语音的使用场景来选择 [开通](https://data.aliyun.com/product/nls?spm=5176.doc30437.2.4.jCYNfj) 哪个服务。

| 服务 | 时效性 | 支持的语音格式 | 支持调用方式 |
| :--- | :--- | :--- | :--- |
| 一句话识别 SDK | 实时识别 | PCM/WAV/SPEEX/OPUS | Java/C++/Android/iOS |
| 实时语音识别 | 实时识别 | PCM/WAV | Java/C++/Android/iOS |
| 录音文件识别 | 非实时，免费版24小时内（这里区分免费版） | 支持大部分常见编码格式，WAV/MP3效果最佳 | HTTP RESTful API |

* 一句话识别
一句话识别 SDK 支持的语言有：汉语普通话，东北、河南、四川等方言，粤语，英语。支持的结果返回模式有"流式"和"普通"两种模式。“流式”模式就是用户一边说话一边返回识别结果，“普通”模式就是用户整句话说完后返回识别结果。

由于语音识别服务为了提高语音识别准确率，对用户的使用场景抽象成了不同的模型，需要我们通过 app\_key 参数来选择具体使用哪个模型。详情见下表。

![image | center](https://gw.alipayobjects.com/zos/skylark/8355e878-2d6d-4e7f-b0bf-6489ca0655b1/2018/png/5dfa81f0-0504-4635-9b23-48ec08b415bc.png "")

集成和 API 的详细使用可见 [官方文档](https://help.aliyun.com/document_detail/30417.html?spm=5176.doc30420.6.548.3bTbet#h2-u91CDu8981u63A5u53E3u8BF4u660E6)。 
简单 Demo 示例：

[embed: 32f3f23c0e1fd7e8ad5f0cb93da64013_3548406206174834041.mp4](https://lark.alipay.com/attachments/lark/2018/mp4/5f1e9720-17e8-41c8-9d96-57187eac779f.mp4 "align:center")


* 实时语音识别
实时语音识别，提供了实时录音识别为文字的能力。我们常用于短视频、直播、会议等应用场景。同样，实时语音识别也提供了多种场景模型的选择，如下表。

![image | center](https://lark.alipay.com/api/filetransfer?url=https%3A%2F%2Fprivate-alipayobjects.alipay.com%2Falipay-rmsdeploy-image%2Fskylark%2Fpng%2F036b8315-b165-4267-af9f-0c871519279f.png "")

 
集成方法、注意事项、API 的使用和完整的示例，可见 [官方文档](https://help.aliyun.com/document_detail/53288.html?spm=5176.doc30417.6.557.SdQKMn#h2-u91CDu8981u63A5u53E3u8BF4u660E)。
简单 Demo 示例：

[embed: 5552fa62455002a7048d2b0203fd07f2_4124779407680446069.mp4](https://lark.alipay.com/attachments/lark/2018/mp4/4993fc11-347a-4e1c-8ff0-d6cb14f0947e.mp4 "align:center")


* 语音合成服务
语音合成服务是在阿里云服务端完成的，客户端将文本内容发送到服务端，服务端合成语音后，以语音数据流的形式返回给客户端 SDK，SDK 再进行播放和存储。阿里云语音合成服务提供了丰富的接口，可以设置不同的发音人、语速、音量等，获取的语音形式也很丰富，包括 PCM、WAV、MP3 等格式。

同样的，语音合成也需要选择场景模型，它所支持的 app\_key 跟前面一句话识别 SDK 的 app\_key 一致。若单独使用「语音合成」，官方推荐使用 nls-service。

![image | center](https://lark.alipay.com/api/filetransfer?url=https%3A%2F%2Fprivate-alipayobjects.alipay.com%2Falipay-rmsdeploy-image%2Fskylark%2Fpng%2Ffb4c77f6-ca2e-4f19-8220-cf4f21bcdcb2.png "")

语音合成的 API 和完整示例，可以参见 [官方的文档。](https://help.aliyun.com/document_detail/30419.html?spm=5176.doc30422.6.573.5ArQSd#h2-u91CDu8981u63A5u53E3u8BF4u660E6)
简单 Demo 示例：

[embed: 9cc33ef06134d9233db2c96cea78c59e_4122492212436376180.mp4](https://lark.alipay.com/attachments/lark/2018/mp4/71a59925-ae00-4c88-9255-e8077acc214a.mp4 "align:center")


## 参考文章
1. [语音识别的技术原理是什么？](https://www.zhihu.com/question/20398418)
2. [玩人工智能的你必须知道的语音识别技术原理](http://www.eepw.com.cn/article/201612/342190.htm)
3. [「翻译」史上最全人工智能产品清单（1/3）](https://zhuanlan.zhihu.com/p/27741586)
4. [“识别一秒钟，线下十年功” —— 语音输入如何一分钟400字](https://zhuanlan.zhihu.com/p/23176664)
5. [阿里云智能语音交互](https://help.aliyun.com/product/30413.html?spm=a2c4g.11186623.3.1.lavc1m)

